"""Demo: pronunciation assessment."""

import argparse
import json
import os
import sys
from typing import Optional
from datetime import datetime

import torch
import torchaudio

from ..config import get_config
from ..Model.model import Model
from ..Evaluation.eval import greedy_ctc_decode
from ..Evaluation.metric import utterance_per
from ..Utils.audio import create_attention_mask, compute_output_lengths
from ..Utils.cmu_dict import get_canonical_phoneme_ids


def load_single_waveform(path: str, config):
    """Îã®Ïùº WAV ÌååÏùº Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨.

    Args:
        path: WAV ÌååÏùº Í≤ΩÎ°ú
        config: ÏÑ§Ï†ï Í∞ùÏ≤¥

    Returns:
        waveform: [1, max_length] ÌòïÌÉúÏùò ÌÖêÏÑú
        audio_lengths: Ïã§Ï†ú Ïò§ÎîîÏò§ Í∏∏Ïù¥ ÌÖêÏÑú
    """
    wav, sr = torchaudio.load(path)
    if wav.size(0) > 1:
        wav = wav.mean(dim=0, keepdim=True)

    if sr != config.sampling_rate:
        wav = torchaudio.functional.resample(wav, sr, config.sampling_rate)

    wav = wav[0]
    length = wav.size(0)

    if length > config.max_length:
        wav = wav[:config.max_length]
        length = config.max_length
    else:
        pad_len = config.max_length - length
        if pad_len > 0:
            wav = torch.nn.functional.pad(wav, (0, pad_len))

    wav = wav.unsqueeze(0)
    audio_lengths = torch.tensor([length], dtype=torch.long)
    return wav, audio_lengths


def run_demo_on_test_dataset(
    checkpoint: str,
    test_data_path: str,
    phoneme_map_path: str,
    sample_index: Optional[int] = None
):
    """Test datasetÏóêÏÑú ÏÉòÌîåÏùÑ ÏÑ†ÌÉùÌïòÏó¨ ÌèâÍ∞Ä.

    Args:
        checkpoint: Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú
        test_data_path: test.json Í≤ΩÎ°ú
        phoneme_map_path: phoneme_to_id.json Í≤ΩÎ°ú
        sample_index: ÌèâÍ∞ÄÌï† ÏÉòÌîåÏùò Ïù∏Îç±Ïä§ (NoneÏù¥Î©¥ Ï≤´ Î≤àÏß∏)
    """
    config = get_config()
    device = config.device if torch.cuda.is_available() else "cpu"

    with open(phoneme_map_path, 'r') as f:
        phoneme_to_id = json.load(f)

    with open(test_data_path, 'r') as f:
        test_data = json.load(f)

    test_items = list(test_data.items())

    if sample_index is None:
        sample_index = 0

    if sample_index >= len(test_items):
        print(f"Error: sample_index {sample_index} out of range (max: {len(test_items) - 1})")
        sys.exit(1)

    wav_path, sample_info = test_items[sample_index]
    text = sample_info.get('wrd', '')

    if not text:
        print(f"Error: No transcript found for sample {sample_index}")
        sys.exit(1)

    canonical_phoneme_ids = get_canonical_phoneme_ids(text, phoneme_to_id)

    if not canonical_phoneme_ids:
        print(f"Error: Could not generate canonical phonemes for text: {text}")
        sys.exit(1)

    model = Model(
        pretrained_model_name=config.pretrained_model_name,
        num_phonemes=config.num_phonemes,
        dropout=config.dropout,
    )

    if not os.path.isfile(checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint}")

    state = torch.load(checkpoint, map_location="cpu")
    if "model_state" in state:
        model.load_state_dict(state["model_state"])
    else:
        model.load_state_dict(state)
    model.to(device)
    model.eval()

    waveforms, audio_lengths = load_single_waveform(wav_path, config)
    waveforms = waveforms.to(device)
    audio_lengths = audio_lengths.to(device)

    input_lengths = compute_output_lengths(model, audio_lengths)
    normalized_lengths = audio_lengths.float() / waveforms.shape[1]
    attention_mask = create_attention_mask(waveforms, normalized_lengths)

    with torch.no_grad():
        outputs = model(waveforms, attention_mask)
        logits = outputs['perceived_logits']
        hyps = greedy_ctc_decode(logits, blank_id=config.blank_id)
        predicted_ids = hyps[0]

    per_value = utterance_per(canonical_phoneme_ids, predicted_ids)

    id_to_phoneme = {v: k for k, v in phoneme_to_id.items()}
    canonical_phonemes = [id_to_phoneme.get(pid, f"<{pid}>") for pid in canonical_phoneme_ids]
    predicted_phonemes = [id_to_phoneme.get(pid, f"<{pid}>") for pid in predicted_ids]

    print("\n=== Demo Result (Test Dataset) ===")
    print(f"Sample index: {sample_index}")
    print(f"Text: {text}")
    print(f"Audio file: {wav_path}")
    print(f"Checkpoint: {checkpoint}")
    print(f"\nCanonical phonemes ({len(canonical_phonemes)}):")
    print(f"  {' '.join(canonical_phonemes)}")
    print(f"Canonical IDs:")
    print(f"  {canonical_phoneme_ids}")
    print(f"\nPredicted phonemes ({len(predicted_ids)}):")
    print(f"  {' '.join(predicted_phonemes)}")
    print(f"Predicted IDs:")
    print(f"  {predicted_ids}")
    print(f"\nPER: {per_value:.4f}")


def run_demo_with_custom_audio(
    audio_path: str,
    text: str,
    checkpoint: str,
    phoneme_map_path: str
):
    """ÏÇ¨Ïö©ÏûêÍ∞Ä Ï†úÍ≥µÌïú Ïò§ÎîîÏò§ ÌååÏùºÍ≥º ÌÖçÏä§Ìä∏Î°ú ÌèâÍ∞Ä.

    Args:
        audio_path: WAV ÌååÏùº Í≤ΩÎ°ú
        text: Î∞úÌôîÌï† ÌÖçÏä§Ìä∏ (canonical)
        checkpoint: Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú
        phoneme_map_path: phoneme_to_id.json Í≤ΩÎ°ú
    """
    config = get_config()
    device = config.device if torch.cuda.is_available() else "cpu"

    with open(phoneme_map_path, 'r') as f:
        phoneme_to_id = json.load(f)

    canonical_phoneme_ids = get_canonical_phoneme_ids(text, phoneme_to_id)

    if not canonical_phoneme_ids:
        print(f"Error: Could not generate canonical phonemes for text: {text}")
        sys.exit(1)

    model = Model(
        pretrained_model_name=config.pretrained_model_name,
        num_phonemes=config.num_phonemes,
        dropout=config.dropout,
    )

    if not os.path.isfile(checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint}")

    state = torch.load(checkpoint, map_location="cpu")
    if "model_state" in state:
        model.load_state_dict(state["model_state"])
    else:
        model.load_state_dict(state)
    model.to(device)
    model.eval()

    waveforms, audio_lengths = load_single_waveform(audio_path, config)
    waveforms = waveforms.to(device)
    audio_lengths = audio_lengths.to(device)

    input_lengths = compute_output_lengths(model, audio_lengths)
    normalized_lengths = audio_lengths.float() / waveforms.shape[1]
    attention_mask = create_attention_mask(waveforms, normalized_lengths)

    with torch.no_grad():
        outputs = model(waveforms, attention_mask)
        logits = outputs['perceived_logits']
        hyps = greedy_ctc_decode(logits, blank_id=config.blank_id)
        predicted_ids = hyps[0]

    per_value = utterance_per(canonical_phoneme_ids, predicted_ids)

    id_to_phoneme = {v: k for k, v in phoneme_to_id.items()}
    canonical_phonemes = [id_to_phoneme.get(pid, f"<{pid}>") for pid in canonical_phoneme_ids]
    predicted_phonemes = [id_to_phoneme.get(pid, f"<{pid}>") for pid in predicted_ids]

    print("\n=== Demo Result (Custom Audio) ===")
    print(f"Text: {text}")
    print(f"Audio file: {audio_path}")
    print(f"Checkpoint: {checkpoint}")
    print(f"\nCanonical phonemes ({len(canonical_phonemes)}):")
    print(f"  {' '.join(canonical_phonemes)}")
    print(f"Canonical IDs:")
    print(f"  {canonical_phoneme_ids}")
    print(f"\nPredicted phonemes ({len(predicted_ids)}):")
    print(f"  {' '.join(predicted_phonemes)}")
    print(f"Predicted IDs:")
    print(f"  {predicted_ids}")
    print(f"\nPER: {per_value:.4f}")


def run_demo_with_participant(
    checkpoint: str,
    phoneme_map_path: str,
    audio_filename: Optional[str] = None
):
    """Ï∞∏Í∞ÄÏûê Î∞úÏùå ÌèâÍ∞Ä Îç∞Î™® (Í≥†Ï†ïÎêú Î¨∏Ïû• ÏÇ¨Ïö©).

    Args:
        checkpoint: Î™®Îç∏ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú
        phoneme_map_path: phoneme_to_id.json Í≤ΩÎ°ú
        audio_filename: recording Ìè¥Îçî ÎÇ¥ Ïò§ÎîîÏò§ ÌååÏùºÎ™Ö (NoneÏù¥Î©¥ Ï≤´ Î≤àÏß∏ wav ÌååÏùº)
    """
    DEMO_TEXT = "The quick brown fox jumps over the lazy dog."
    RECORDING_DIR = "recording"

    config = get_config()
    device = config.device if torch.cuda.is_available() else "cpu"

    # recording Ìè¥ÎçîÏóêÏÑú Ïò§ÎîîÏò§ ÌååÏùº Ï∞æÍ∏∞
    if audio_filename is None:
        if not os.path.exists(RECORDING_DIR):
            print(f"Error: Recording directory '{RECORDING_DIR}' does not exist")
            sys.exit(1)
        
        wav_files = [f for f in os.listdir(RECORDING_DIR) if f.endswith('.wav')]
        if not wav_files:
            print(f"Error: No WAV files found in '{RECORDING_DIR}' directory")
            print(f"Please record '{DEMO_TEXT}' and save it as a WAV file in the '{RECORDING_DIR}' folder")
            sys.exit(1)
        
        audio_filename = wav_files[0]
        print(f"Using audio file: {audio_filename}")
    
    audio_path = os.path.join(RECORDING_DIR, audio_filename)
    
    if not os.path.isfile(audio_path):
        print(f"Error: Audio file not found: {audio_path}")
        sys.exit(1)

    with open(phoneme_map_path, 'r') as f:
        phoneme_to_id = json.load(f)

    canonical_phoneme_ids = get_canonical_phoneme_ids(DEMO_TEXT, phoneme_to_id)

    if not canonical_phoneme_ids:
        print(f"Error: Could not generate canonical phonemes for text: {DEMO_TEXT}")
        sys.exit(1)

    model = Model(
        pretrained_model_name=config.pretrained_model_name,
        num_phonemes=config.num_phonemes,
        dropout=config.dropout,
    )

    if not os.path.isfile(checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint}")

    state = torch.load(checkpoint, map_location="cpu")
    if "model_state" in state:
        model.load_state_dict(state["model_state"])
    else:
        model.load_state_dict(state)
    model.to(device)
    model.eval()

    waveforms, audio_lengths = load_single_waveform(audio_path, config)
    waveforms = waveforms.to(device)
    audio_lengths = audio_lengths.to(device)

    input_lengths = compute_output_lengths(model, audio_lengths)
    normalized_lengths = audio_lengths.float() / waveforms.shape[1]
    attention_mask = create_attention_mask(waveforms, normalized_lengths)

    with torch.no_grad():
        outputs = model(waveforms, attention_mask)
        logits = outputs['perceived_logits']
        hyps = greedy_ctc_decode(logits, blank_id=config.blank_id)
        predicted_ids = hyps[0]

    per_value = utterance_per(canonical_phoneme_ids, predicted_ids)

    id_to_phoneme = {v: k for k, v in phoneme_to_id.items()}
    canonical_phonemes = [id_to_phoneme.get(pid, f"<{pid}>") for pid in canonical_phoneme_ids]
    predicted_phonemes = [id_to_phoneme.get(pid, f"<{pid}>") for pid in predicted_ids]

    print("\n" + "="*60)
    print("       PRONUNCIATION ASSESSMENT DEMO")
    print("="*60)
    print(f"\nüìù Reference Text:")
    print(f"   {DEMO_TEXT}")
    print(f"\nüé§ Audio File: {audio_path}")
    print(f"ü§ñ Model Checkpoint: {checkpoint}")
    print(f"\n{'‚îÄ'*60}")
    print(f"üìä Canonical Phonemes ({len(canonical_phonemes)}):")
    print(f"   {' '.join(canonical_phonemes)}")
    print(f"\nüîç Predicted Phonemes ({len(predicted_ids)}):")
    print(f"   {' '.join(predicted_phonemes)}")
    print(f"\n{'‚îÄ'*60}")
    print(f"üìà Phoneme Error Rate (PER): {per_value:.4f} ({per_value*100:.2f}%)")
    
    # Í∞ÑÎã®Ìïú ÌèâÍ∞Ä Î©îÏãúÏßÄ
    if per_value < 0.1:
        rating = "Excellent! üåüüåüüåü"
    elif per_value < 0.2:
        rating = "Good! üåüüåü"
    elif per_value < 0.3:
        rating = "Fair üåü"
    else:
        rating = "Needs improvement"
    
    print(f"üí¨ Assessment: {rating}")
    print("="*60 + "\n")
    
    # Í≤∞Í≥º Ï†ÄÏû•
    save_result(
        audio_filename=audio_filename,
        audio_path=audio_path,
        text=DEMO_TEXT,
        canonical_phonemes=canonical_phonemes,
        predicted_phonemes=predicted_phonemes,
        canonical_ids=canonical_phoneme_ids,
        predicted_ids=predicted_ids,
        per_value=per_value,
        rating=rating,
        checkpoint=checkpoint
    )


def save_result(
    audio_filename: str,
    audio_path: str,
    text: str,
    canonical_phonemes: list,
    predicted_phonemes: list,
    canonical_ids: list,
    predicted_ids: list,
    per_value: float,
    rating: str,
    checkpoint: str
):
    """ÌèâÍ∞Ä Í≤∞Í≥ºÎ•º JSON ÌååÏùºÎ°ú Ï†ÄÏû•.
    
    Args:
        audio_filename: Ïò§ÎîîÏò§ ÌååÏùºÎ™Ö
        audio_path: Ïò§ÎîîÏò§ ÌååÏùº Ï†ÑÏ≤¥ Í≤ΩÎ°ú
        text: ÌèâÍ∞Ä ÎåÄÏÉÅ ÌÖçÏä§Ìä∏
        canonical_phonemes: Ï†ïÎãµ ÏùåÏÜå Î¶¨Ïä§Ìä∏
        predicted_phonemes: ÏòàÏ∏° ÏùåÏÜå Î¶¨Ïä§Ìä∏
        canonical_ids: Ï†ïÎãµ ÏùåÏÜå ID Î¶¨Ïä§Ìä∏
        predicted_ids: ÏòàÏ∏° ÏùåÏÜå ID Î¶¨Ïä§Ìä∏
        per_value: Phoneme Error Rate
        rating: ÌèâÍ∞Ä Îì±Í∏â
        checkpoint: ÏÇ¨Ïö©Ìïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Í≤ΩÎ°ú
    """
    # results Ìè¥Îçî ÏÉùÏÑ±
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)
    
    # ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÏÉùÏÑ±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ÌååÏùºÎ™ÖÏóêÏÑú ÌôïÏû•Ïûê Ï†úÍ±∞
    audio_name = os.path.splitext(audio_filename)[0]
    
    # Í≤∞Í≥º ÌååÏùºÎ™Ö ÏÉùÏÑ±
    result_filename = f"{audio_name}_{timestamp}.json"
    result_path = os.path.join(results_dir, result_filename)
    
    # Í≤∞Í≥º Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ±
    result_data = {
        "timestamp": datetime.now().isoformat(),
        "audio_file": audio_path,
        "audio_filename": audio_filename,
        "reference_text": text,
        "checkpoint": checkpoint,
        "canonical": {
            "phonemes": canonical_phonemes,
            "ids": canonical_ids,
            "count": len(canonical_phonemes)
        },
        "predicted": {
            "phonemes": predicted_phonemes,
            "ids": predicted_ids,
            "count": len(predicted_phonemes)
        },
        "evaluation": {
            "per": round(per_value, 4),
            "per_percentage": round(per_value * 100, 2),
            "rating": rating
        }
    }
    
    # JSON ÌååÏùºÎ°ú Ï†ÄÏû•
    with open(result_path, 'w', encoding='utf-8') as f:
        # Î®ºÏ†Ä Îì§Ïó¨Ïì∞Í∏∞Îêú JSONÏúºÎ°ú Î≥ÄÌôò
        json_str = json.dumps(result_data, indent=2, ensure_ascii=False)
        
        # phonemesÏôÄ ids Î∞∞Ïó¥ÏùÑ Ìïú Ï§ÑÎ°ú Î≥ÄÌôò
        import re
        # "phonemes": [ ... ] Ìå®ÌÑ¥ÏùÑ Ï∞æÏïÑÏÑú Ìïú Ï§ÑÎ°ú Î≥ÄÍ≤Ω
        json_str = re.sub(r'"phonemes":\s*\[\s*([^\]]+?)\s*\]', 
                         lambda m: '"phonemes": [' + ', '.join(s.strip() for s in m.group(1).split(',')) + ']',
                         json_str, flags=re.DOTALL)
        # "ids": [ ... ] Ìå®ÌÑ¥ÏùÑ Ï∞æÏïÑÏÑú Ìïú Ï§ÑÎ°ú Î≥ÄÍ≤Ω
        json_str = re.sub(r'"ids":\s*\[\s*([^\]]+?)\s*\]',
                         lambda m: '"ids": [' + ', '.join(s.strip() for s in m.group(1).split(',')) + ']',
                         json_str, flags=re.DOTALL)
        
        f.write(json_str)
    
    print(f"üíæ Result saved to: {result_path}")


def main():
    """Demo mode ÏßÑÏûÖÏ†ê."""
    parser = argparse.ArgumentParser(description="Demo pronunciation assessment")

    parser.add_argument(
        "--mode",
        type=str,
        required=True,
        choices=["test_dataset", "custom", "participant"],
        help="Demo mode: test_dataset, custom, or participant"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        required=True,
        help="Model checkpoint path"
    )
    parser.add_argument(
        "--phoneme_map",
        type=str,
        default="data/phoneme_to_id.json",
        help="Path to phoneme_to_id.json"
    )

    parser.add_argument(
        "--test_data",
        type=str,
        default="data/test.json",
        help="Path to test.json (for test_dataset mode)"
    )
    parser.add_argument(
        "--sample_index",
        type=int,
        default=0,
        help="Sample index to use from test dataset"
    )

    parser.add_argument(
        "--audio",
        type=str,
        help="Audio file path (for custom mode) or filename in recording/ (for participant mode)"
    )
    parser.add_argument(
        "--text",
        type=str,
        help="Transcript text (for custom mode)"
    )

    args = parser.parse_args()

    if args.mode == "test_dataset":
        run_demo_on_test_dataset(
            checkpoint=args.checkpoint,
            test_data_path=args.test_data,
            phoneme_map_path=args.phoneme_map,
            sample_index=args.sample_index
        )
    elif args.mode == "custom":
        if not args.audio or not args.text:
            print("Error: --audio and --text are required for custom mode")
            sys.exit(1)
        run_demo_with_custom_audio(
            audio_path=args.audio,
            text=args.text,
            checkpoint=args.checkpoint,
            phoneme_map_path=args.phoneme_map
        )
    elif args.mode == "participant":
        run_demo_with_participant(
            checkpoint=args.checkpoint,
            phoneme_map_path=args.phoneme_map,
            audio_filename=args.audio
        )


if __name__ == "__main__":
    main()
